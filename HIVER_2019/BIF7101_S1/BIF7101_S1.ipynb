{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**BIF7010 | Bioinformatique des structures**\n",
    "\n",
    "UQAM - Hiver 2019\n",
    "\n",
    "Amine Remita\n",
    "\n",
    "\n",
    "**Apprentissage automatique - Partie 1**\n",
    "\n",
    "\n",
    "Introduction à l'apprentissage supervisé\n",
    "\n",
    "\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "BIF7101_S1 by Amine Remita is licensed under a Creative Commons Attribution-\n",
    "ShareAlike 4.0 International License.\n",
    "\n",
    "\n",
    "Quelques sous-sections et figures sont prises (avec autorisation) à partir des cours de Mohamed Bouguessa Ph.D. (DIC9370) et Ahmed Halioui Ph.D. (BIF7101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Apprentissage naturel\n",
    "\n",
    "* La faculté d'apprendre de ces expériences passées et de s'adapter est une caractéristique essentielle des formes de vies\n",
    "* Elle est essentielle dans les premières étapes de la vie pour apprendre des choses aussi fondamentales que reconnaître une voix, un visage familier, apprendre à comprendre ce qui est dit, à marcher et à parler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apprentissage artificiel (machine learning)\n",
    "\n",
    "* Une tentative de comprendre et reproduire cette faculté d'apprentissage dans des systèmes artificiels\n",
    "* Concevoir des algorithmes capables à partir d'un nombre important d'exemples (expériences passées) d'en assimiler la nature afin de pouvoir appliquer ce qu'ils ont appris aux cas futurs\n",
    "* Objectif de l'apprentissage : déterminer la relation entre les exemple et leurs catégories pour la prédiction et la découverte des connaissances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Un programme possède des capacités d'apprentissage si au cours du traitement d'exemples représentatifs de données il est capable de construire et d'utiliser une représentation de ce traitement en vue de son exploitation\n",
    "* => Élaboration d'un modèle pour la prédiction et la découverte des connaissances\n",
    "* Modèle = Description formelle des relations qui existent entre l'ensemble des attributs qui décrivent les données à traiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications de l'apprentissage artificiel\n",
    "\n",
    "* Traitement automatique du langage naturel\n",
    "    * Classification de texte\n",
    "    * Identification de langues\n",
    "    * Identification des auteurs\n",
    "\n",
    "\n",
    "* Bioinformatique et biologie computationnnelle\n",
    "    * Identification et prédiction des gènes\n",
    "    * Annotation fonctionnelle des séquences\n",
    "    * Analyse des données d'expression de génes\n",
    "\n",
    "\n",
    "* Imagerie Médicale\n",
    "\n",
    "\n",
    "* Reconnaissance automatique de la parole\n",
    "\n",
    "\n",
    "* Robotique\n",
    "    * Véhicule autonome\n",
    "    * Planification automatique\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logiciels d'apprentissage automatique\n",
    "\n",
    "![](figs/ml_software.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "![](figs/scikit-learn.png)\n",
    "\n",
    "#### Logiciel\n",
    "* open-source\n",
    "* gratuit\n",
    "* python\n",
    "* implémente des algorithmes d'apprentissage automatique\n",
    "* https://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environnement\n",
    "\n",
    "* Python\n",
    "* IPython\n",
    "* Scipy et Numpy\n",
    "* Matplotlib\n",
    "* Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fouille de données\n",
    "\n",
    "![](figs/data_mining_chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Les données\n",
    "\n",
    "Dans un problème d'apprentissage automatique, les informations caractérisant une étude sont présentées sous la forme d'**attributs** et d'**objets**\n",
    "\n",
    "**Attribut** : un discripteur d'une entité (dimension, variable)\n",
    "\n",
    "**Objet** : un ensemble d'attributs (enregistrement, exemple, vecteur, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du jeu de données breast cancer\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "type(cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Matrice objets/attributs\n",
    "\n",
    "X = cancer.data\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Valeurs du premier objet\n",
    "\n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation de la matrice objets/attributs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = sns.heatmap(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Noms des attributs\n",
    "print(cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Noms des classes cibles\n",
    "print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Liste des classes des objets \n",
    "\n",
    "y = cancer.target\n",
    "print(y[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types d'attributs\n",
    "\n",
    "**Attribut discret**\n",
    "\n",
    "\n",
    "* Numérique discret : la valeur de l'attribut est un entier\n",
    "* Catégorie/symbole exemple {rouge, vert, bleu}\n",
    "* Données binaires\n",
    "\n",
    "**Attribut numérique continu**\n",
    "\n",
    "\n",
    "La valeur de l'attribut peut prendre une valeur numérique\n",
    "* Montant du compte en banque\n",
    "* poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prétraitement des données\n",
    "\n",
    "* Le prétraitement des données est crucial dans le processus de l'apprentissage automatique car les résultats dépendent de leur qualité\n",
    "\n",
    "* Les données réelles sont souvent :\n",
    "  * Incomplètes\n",
    "  * Bruitées\n",
    "  * Incohérentes\n",
    "\n",
    "**Principales étapes de prétraitement**\n",
    "\n",
    "* Intégration\n",
    "* Nettoyage\n",
    "* Transformation\n",
    "* Réduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integration\n",
    "\n",
    "Combiner des sources de données différentes dans une seule structure\n",
    "* Détecter et résoudre les conflits de valeurs\n",
    "  * Dans un seul attribut on peut avoir plusieurs mesures différentes (cm et pouces)\n",
    "\n",
    "* Gestion de la redondance\n",
    "  * Le même attribut peut avoir des noms différents\n",
    "  * Un attribut peut être déduit d'un autre\n",
    "  * Solution : analyse de corrélation entre les attributs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nettoyage\n",
    "\n",
    "#### Données manquantes\n",
    "\n",
    "Données non disponible / certaines attributs n'ont pas de valeurs\n",
    "\n",
    "1. Causes\n",
    " \n",
    "    * Mauvais fonctionnement de l'équipement\n",
    "    * Incohérences avec d'autres données => supprimées\n",
    "    * Non saisies car non ou mal comprises (considérées peu importantes au moment de la saisie)\n",
    "\n",
    "2. Solutions\n",
    "\n",
    "Ces données doivent être inférées/imputées\n",
    "* Ignorer le tuple\n",
    "* Utiliser une constante globale\n",
    "* Utiliser la moyenne de l'attribut\n",
    "* Utiliser la valeur la plus probable : formule Bayésienne ou arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# From https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "data = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Données bruitées\n",
    "\n",
    "Bruit :\n",
    "  * Une erreur ou une valeur aléatoire (excessive)\n",
    "  * Un objet qui a des caractéristiques complètement difféfrentes du reste de l'ensemble de données\n",
    "\n",
    "1. Causes\n",
    "  * Instrument de mesure défectueux\n",
    "  * Problème de saisie\n",
    "  * Problème de transmission\n",
    "  \n",
    "2. Solutions\n",
    "  * Clustering\n",
    "  * Écart interquartile pour identifier les valeurs aberrantes (outliers)\n",
    "    * $IQR=Q3-Q1$\n",
    "    * Inférieur à $Q1-(\\alpha \\times IQR)$\n",
    "    * Supérieur à $Q3+(\\alpha \\times IQR)$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "D = X[:, 23:24]\n",
    "transformer = RobustScaler().fit(D)\n",
    "\n",
    "D_t = transformer.transform(D)\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "\n",
    "axs[0].hist(D, bins='auto', label='x')\n",
    "axs[1].hist(D_t, bins='auto', label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformation\n",
    "\n",
    "#### Standardisation et normalisation\n",
    "\n",
    "* **Z-score**\n",
    "  * Même ordre de grandeurs pour les valeurs des attributs\n",
    "  * $v' = \\frac{v - \\mu_A}{\\sigma_A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "D = X[:, 23:24]\n",
    "\n",
    "scaler = StandardScaler().fit(D)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)\n",
    "\n",
    "D_t = scaler.transform(D)\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "axs[0].hist(D, bins='auto')\n",
    "axs[1].hist(D_t, bins='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Méthode min-max**\n",
    "  * Mise à l'échelle pour avoir un petit intervalle spécifié\n",
    "  * $v' = \\frac{v - min_A}{max_A - min_A} (new\\_max_A - new\\_min_A) + new\\_min_A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "D_train = X[:, 23:24]\n",
    "D_test = X[:, 24:25]\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "D_train_minmax = min_max_scaler.fit_transform(D_train)\n",
    "#print(X_train_minmax)\n",
    "\n",
    "D_test_minmax = min_max_scaler.transform(D_test)\n",
    "#print(X_test_minmax)\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "axs[0].hist(D_train_minmax, bins='auto')\n",
    "axs[1].hist(D_test_minmax, bins='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Discrétisation\n",
    "* Attributs numérique => attributs nominaux\n",
    "* Découper le domaine de variation en un nombre fini d'intervalles\n",
    "* Discrétisation supervisée (Découper en K domaines égaux)\n",
    "* Discrétisation non  supervisée (Découper itérativement en 2 sous ensembles jusqu'à une certaine condition d'arrêt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "D = [[-2, 1, -4,   -1],\n",
    "     [-1, 2, -3, -0.5],\n",
    "     [ 0, 3, -2,  0.5],\n",
    "     [ 1, 4, -1,    2]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "est.fit(D)  \n",
    "\n",
    "Xt = est.transform(D)\n",
    "Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Réduction de la dimension\n",
    "\n",
    "* Certains attributs contiennent de l'information non pertinente et rend l'analyse des données plus complexe\n",
    "* La présence des attributs non pertinents augmente potentiellement le temps d'exécution des algorithmes\n",
    "* Solution : Réduction de la dimension\n",
    "  \n",
    "  Obtenir une représentation réduite du jeu de données, plus petit en volume mais qui produit (ou presque) les mêmes résultats analytiques\n",
    "    * Analyse en composantes principales (PCA) : création d'un nouvel attribut à partir des attributs originaux\n",
    "    * Techniques de sélection d'attributs (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apprentissage automatique\n",
    "\n",
    "1. *Apprentissage supervisé* : la décision est connue\n",
    "    * Classification : décision / classe = catégorie\n",
    "    * Régression : décision / classe = nombre continu\n",
    "2. *Apprentissage non supervisé* : la décision est inconnue\n",
    "    * Partitionnement (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apprentissage supervisé\n",
    "\n",
    "* Entrée : base de données d'apprentissage, ensemble d'exemples\n",
    "* Trouver une fonction (un modèle) $c : X -> Y$ qui approxime et généralise au mieux la relation entre les exemples $x_i$ et leurs catégories $y_i$\n",
    "\n",
    "* But:\n",
    "    * Modèle de prédiciton : classificaiton de nouvelle données\n",
    "    * Schéma explicatif : aide à comprendre les relations qui existent entre les entrées et les sorties\n",
    "    * Régression : approximation de fonction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/fig_ml_supervised.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pld\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_decision(data, targets, clf):\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "            np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "    D_train, D_test, yd_train, yd_test = \\\n",
    "            train_test_split(data, targets, test_size=.4, random_state=42)\n",
    "\n",
    "    cm = pld.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    clf.fit(D_train, yd_train)\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    pld.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    pld.scatter(D_train[:, 0], D_train[:, 1], c=yd_train, cmap=cm_bright,\n",
    "            edgecolors='k')\n",
    "\n",
    "    pld.scatter(D_test[:, 0], D_test[:, 1], c=yd_test, cmap=cm_bright,\n",
    "            edgecolors='k', alpha=0.6)\n",
    "\n",
    "    pld.xlim(xx.min(), xx.max())\n",
    "    pld.ylim(yy.min(), yy.max())\n",
    "    pld.xticks(())\n",
    "    pld.yticks(())\n",
    "\n",
    "    pld.tight_layout()\n",
    "    pld.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Algorithmes d'apprentissage supervisé**\n",
    "\n",
    "### Classification bayésienne naïve\n",
    "\n",
    "* Calcule la probabilité d'appartenance d'un objet $X$ à une classe $y$\n",
    "\n",
    "* Modèlise la distribution co-jointe $P(Y,\\, X)$\n",
    "    \n",
    "    => **Modèle génératif**\n",
    "\n",
    "\n",
    "* Se base sur le **théorème de Bayes** : \n",
    "\n",
    "$P(Y=benign|X) = \\frac{P(Y=benign, \\,X)}{P(X)} = \\frac{P(X|Y=benign) \\, P(Y=benign)}{P(X)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "* La classification d’un nouvel objet $X^{'}$ est effectuée par l'identification de la classe $Y^{'}$ qui maximise la probabilité *a posteriori* $P(Y^{'}|X^{'})$\n",
    "\n",
    "\n",
    "* $Y^{'} = arg\\,max_{y\\in \\{benign,\\, malignant\\}} \\, P(Y=y|X^{'})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Apprentissage\n",
    "\n",
    "* Estimer les probabailités *a posteriori* $P(Y=benign|X)$ et $P(Y=malignant|X)$ par le théorème de Bayes\n",
    "* Supposer que les attributs sont conditionnellement indépendants étant donnée une classe $y$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) = P(X|Y=benign) \\, P(Y=benign) \\, /\\, P(X)$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) = \\prod_i P(x_i|Y=benign) \\, P(Y=benign) \\, /\\,\\, P(X)$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) \\propto \\prod_i P(x_i|Y=benign) \\, P(Y=benign)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Apprentissage\n",
    "\n",
    "* $P(Y=benign)$ peut être estimée par Maximum de vraisemblance\n",
    "\n",
    "* **Bayes naif gaussien**\n",
    "    * $P(x_i|Y=benign) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{benign}}} \\exp\\left(-\\frac{(x_i - \\mu_{benign})^2}{2\\sigma^2_{benign}}\\right)$\n",
    "    \n",
    "\n",
    "* **Bayes naif multinomial**\n",
    "    * $P(x_i|Y=benign) = \\frac{ N_{benign} + \\alpha}{N_y + \\alpha n}$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "X_new = min_max_scaler.fit_transform(X_reduced)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_new, y, test_size=.4, random_state=42)\n",
    "\n",
    "# Instanciation du classifieur\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Apprentissage/entrainement\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#plot_decision(X[:, 0:12], y ,clf)\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciation du classifieur\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Apprentissage/entrainement\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#plot_decision(X[:, 0:12], y ,clf)\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Régression logistique\n",
    "\n",
    "* Calcule la probabilité d'appartenance d'un objet $X$ à une classe $y$\n",
    "\n",
    "* Modèlise directement la probabilité  $P(Y|\\,X)$\n",
    "    \n",
    "    => **Modèle discriminatif**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "* La classification d’un nouvel objet $X^{'}$ est effectuée par l'identification de la classe $Y^{'}$ qui maximise la probabilité *a posteriori* $P(Y^{'}|X^{'})$\n",
    "\n",
    "\n",
    "* $Y^{'} = arg\\,max_{y\\in \\{benign,\\, malignant\\}} \\, P(Y=y|X^{'})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme d'apprentissage \n",
    "\n",
    "$P(Y=benign|X) = \\frac{1}{1 + exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}$\n",
    "\n",
    "$P(Y=malignant|X) = \\frac{exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}{1 + exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}$\n",
    "\n",
    "\n",
    "* Estimation des poids **w** à partir des données d'apprentissage se fait par optimisation d'une fonction objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=10, penalty='l2', solver='saga', max_iter=10000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Séparateurs à vaste marge (SVM)\n",
    "\n",
    "* SVM séparent les objets en deux classes par un hyperplan en utilisant des exemples essentiels appelés vecteurs de support\n",
    "* SVM cherchent à trouver l’hyperplan qui minimise le risque empirique de classification (le nombre d’exemples de test mal classés)\n",
    "* L’hyperplan avec une marge maximale minimise ce risque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/svm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# linéaire\n",
    "clf = SVC(kernel=\"linear\", C=0.025)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Non linéaire\n",
    "# Noyau Radial Basis Function\n",
    "clf = SVC(kernel=\"rbf\", gamma=2, C=1)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Arbres de décision\n",
    "\n",
    "\n",
    "#### Algorithme d'apprentissage \n",
    "À partir des données d'apprentissage, identifier des règles de décision qui permettent la séparation des classes\n",
    "\n",
    "#### Structure de l'arbre de décision\n",
    "* Noeud interne correspond à un test sur un attribut\n",
    "* Branche représente le résultats du test\n",
    "* Feuille contient une classe (une classe peut correspondre à plusieurs feuilles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/build_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classement des attributs\n",
    "\n",
    "La probabilité qu'un objet de classe $C_i$ appartient à $X$ ($p_i$) est estimée par $|C_{i,D}|/|D|$\n",
    "\n",
    "1. **Gain d'information** (utilisé par ID3)\n",
    "\n",
    "$Gain(X, A) = Entropie(X) - \\sum_{v\\in valeur(A)}^{}\\frac{|X_v|}{|X|} \\times Entropie(X_v)$\n",
    "\n",
    "$Entropie(X) = -\\sum_{i=1}^{m} p_i log_2(p_i)$\n",
    "\n",
    "2. **indice de Gini** (utilisé par CART)\n",
    "\n",
    "$Gini(A) = \\sum_{i=1}^{v}\\frac{|X_v|}{|X|} \\times Gini(X_v)$\n",
    "\n",
    "$Gini(X_v) = 1-\\sum_{i=1}^{m} p_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "La classification des objets se fait par une séquence de tests successifs de l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K plus proches voisins (KNN)\n",
    "\n",
    "* Conserve les objets d'apprentissage pour les utiliser dans la classification des objets inconnus\n",
    "* La classification se fait la recherche et la comparaison avec les $k$ objets proches\n",
    "* la proximité est définie par une distance \n",
    "* Exemple : distance euclidienne:\n",
    "\n",
    "Si $X_1 = (x_{11}, x_{12},..., x_{1n})$ et $X_2 = (x_{21}, x_{22},..., x_{2n})$\n",
    "\n",
    "$dist(X_1, X_2) = \\sqrt{\\sum_{i=1}^{n}(x_{1i} - x_{2i})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(alpha=1, activation='relu', solver='sgd',\\\n",
    "                    max_iter=1000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X_new, y ,clf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
