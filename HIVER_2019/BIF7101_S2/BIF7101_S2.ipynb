{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**BIF7010 | Bioinformatique des structures**\n",
    "\n",
    "UQAM - Hiver 2019\n",
    "\n",
    "Amine Remita\n",
    "\n",
    "\n",
    "**Apprentissage automatique - Partie 2**\n",
    "\n",
    "\n",
    "* Apprentissage supervisé\n",
    "\n",
    "* Apprentissage non supervisé\n",
    "\n",
    "* Évaluation de l'apprentissage\n",
    "\n",
    "\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "BIF7101_S2 by Amine Remita is licensed under a Creative Commons Attribution-\n",
    "ShareAlike 4.0 International License.\n",
    "\n",
    "\n",
    "Quelques sous-sections et figures sont prises (avec autorisation) à partir des cours de Mohamed Bouguessa Ph.D. (DIC9370) et Ahmed Halioui Ph.D. (BIF7101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apprentissage supervisé\n",
    "\n",
    "* Entrée : base de données d'apprentissage, ensemble d'exemples\n",
    "* Trouver une fonction (un modèle) $c : X -> Y$ qui approxime et généralise au mieux la relation entre les exemples $x_i$ et leurs catégories $y_i$\n",
    "\n",
    "* But:\n",
    "    * Modèle de prédiciton : classificaiton de nouvelle données\n",
    "    * Schéma explicatif : aide à comprendre les relations qui existent entre les entrées et les sorties\n",
    "    * Régression : approximation de fonction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![ml_supervised](figs/fig_ml_supervised.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## initialisation\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = np.array(iris.target)\n",
    "\n",
    "# fusioner les deux classes versicolor et virginica\n",
    "# pour avoir seulement deux classes\n",
    "\n",
    "X = X[y!=2]\n",
    "y = y[y!=2]\n",
    "\n",
    "# On garde les deux premiers attributs \n",
    "X = X[:,0:2]\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fonction pour ploter la bordure de decision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pld\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_decision(data, targets, clf):\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "            np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "    D_train, D_test, yd_train, yd_test = \\\n",
    "            train_test_split(data, targets, test_size=.4, random_state=42)\n",
    "\n",
    "    cm = pld.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    clf.fit(D_train, yd_train)\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    pld.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    pld.scatter(D_train[:, 0], D_train[:, 1], c=yd_train, cmap=cm_bright,\n",
    "            edgecolors='k')\n",
    "\n",
    "    pld.scatter(D_test[:, 0], D_test[:, 1], c=yd_test, cmap=cm_bright,\n",
    "            edgecolors='k', alpha=0.6)\n",
    "\n",
    "    pld.xlim(xx.min(), xx.max())\n",
    "    pld.ylim(yy.min(), yy.max())\n",
    "    pld.xticks(())\n",
    "    pld.yticks(())\n",
    "\n",
    "    pld.tight_layout()\n",
    "    pld.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K plus proches voisins (KNN)\n",
    "\n",
    "* Conserve les objets d'apprentissage pour les utiliser dans la classification des objets inconnus\n",
    "* La classification se fait la recherche et la comparaison avec les $k$ objets proches\n",
    "* la proximité est définie par une distance \n",
    "* Exemple : distance euclidienne:\n",
    "\n",
    "Si $X_1 = (x_{11}, x_{12},..., x_{1n})$ et $X_2 = (x_{21}, x_{22},..., x_{2n})$\n",
    "\n",
    "$dist(X_1, X_2) = \\sqrt{\\sum_{i=1}^{n}(x_{1i} - x_{2i})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification bayésienne naïve\n",
    "\n",
    "* Calcule la probabilité d'appartenance d'un objet $X$ à une classe $y$\n",
    "\n",
    "* Modèlise la distribution co-jointe $P(Y,\\, X)$\n",
    "    \n",
    "    => **Modèle génératif**\n",
    "\n",
    "\n",
    "* Se base sur le **théorème de Bayes** : \n",
    "\n",
    "$P(Y=benign|X) = \\frac{P(Y=benign, \\,X)}{P(X)} = \\frac{P(X|Y=benign) \\, P(Y=benign)}{P(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "* La classification d’un nouvel objet $X^{'}$ est effectuée par l'identification de la classe $Y^{'}$ qui maximise la probabilité *a posteriori* $P(Y^{'}|X^{'})$\n",
    "\n",
    "\n",
    "* $Y^{'} = arg\\,max_{y\\in \\{benign,\\, malignant\\}} \\, P(Y=y|X^{'})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Apprentissage\n",
    "\n",
    "* Estimer les probabailités *a posteriori* $P(Y=benign|X)$ et $P(Y=malignant|X)$ par le théorème de Bayes\n",
    "* Supposer que les attributs sont conditionnellement indépendants étant donnée une classe $y$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) = P(X|Y=benign) \\, P(Y=benign) \\, /\\, P(X)$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) = \\prod_i P(x_i|Y=benign) \\, P(Y=benign) \\, /\\,\\, P(X)$\n",
    "\n",
    "\n",
    "* $P(Y=benign|X) \\propto \\prod_i P(x_i|Y=benign) \\, P(Y=benign)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Apprentissage\n",
    "\n",
    "* $P(Y=benign)$ peut être estimée par Maximum de vraisemblance\n",
    "\n",
    "* **Bayes naif gaussien**\n",
    "    * $P(x_i|Y=benign) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{benign}}} \\exp\\left(-\\frac{(x_i - \\mu_{benign})^2}{2\\sigma^2_{benign}}\\right)$\n",
    "    \n",
    "\n",
    "* **Bayes naif multinomial**\n",
    "    * $P(x_i|Y=benign) = \\frac{ N_{benign} + \\alpha}{N_y + \\alpha n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Instanciation du classifieur\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Apprentissage/entrainement\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#plot_decision(X[:, 0:12], y ,clf)\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciation du classifieur\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Apprentissage/entrainement\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#plot_decision(X[:, 0:12], y ,clf)\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Régression logistique\n",
    "\n",
    "* Calcule la probabilité d'appartenance d'un objet $X$ à une classe $y$\n",
    "\n",
    "* Modèlise directement la probabilité  $P(Y|\\,X)$\n",
    "    \n",
    "    => **Modèle discriminatif**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "* La classification d’un nouvel objet $X^{'}$ est effectuée par l'identification de la classe $Y^{'}$ qui maximise la probabilité *a posteriori* $P(Y^{'}|X^{'})$\n",
    "\n",
    "\n",
    "* $Y^{'} = arg\\,max_{y\\in \\{benign,\\, malignant\\}} \\, P(Y=y|X^{'})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme d'apprentissage \n",
    "\n",
    "$P(Y=benign|X) = \\frac{1}{1 + exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}$\n",
    "\n",
    "$P(Y=malignant|X) = \\frac{exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}{1 + exp(w_0 + \\sum_{i=1}^{n}w_i X_i)}$\n",
    "\n",
    "\n",
    "* Estimation des poids **w** à partir des données d'apprentissage se fait par optimisation d'une fonction objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=10, penalty='l2', solver='saga', max_iter=10000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Séparateurs à vaste marge (SVM)\n",
    "\n",
    "* SVM séparent les objets en deux classes par un hyperplan en utilisant des exemples essentiels appelés vecteurs de support\n",
    "* SVM cherchent à trouver l’hyperplan qui minimise le risque empirique de classification (le nombre d’exemples de test mal classés)\n",
    "* L’hyperplan avec une marge maximale minimise ce risque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![svm](figs/svm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# linéaire\n",
    "clf = SVC(kernel=\"linear\", C=0.025)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Non linéaire\n",
    "# Noyau Radial Basis Function\n",
    "clf = SVC(kernel=\"rbf\", gamma=2, C=1)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Arbres de décision\n",
    "\n",
    "\n",
    "#### Algorithme d'apprentissage \n",
    "À partir des données d'apprentissage, identifier des règles de décision qui permettent la séparation des classes\n",
    "\n",
    "#### Structure de l'arbre de décision\n",
    "* Noeud interne correspond à un test sur un attribut\n",
    "* Branche représente le résultats du test\n",
    "* Feuille contient une classe (une classe peut correspondre à plusieurs feuilles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![build tree](figs/build_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classement des attributs\n",
    "\n",
    "La probabilité qu'un objet de classe $C_i$ appartient à $X$ ($p_i$) est estimée par $|C_{i,D}|/|D|$\n",
    "\n",
    "1. **Gain d'information** (utilisé par ID3)\n",
    "\n",
    "$Gain(X, A) = Entropie(X) - \\sum_{v\\in valeur(A)}^{}\\frac{|X_v|}{|X|} \\times Entropie(X_v)$\n",
    "\n",
    "$Entropie(X) = -\\sum_{i=1}^{m} p_i log_2(p_i)$\n",
    "\n",
    "2. **indice de Gini** (utilisé par CART)\n",
    "\n",
    "$Gini(A) = \\sum_{i=1}^{v}\\frac{|X_v|}{|X|} \\times Gini(X_v)$\n",
    "\n",
    "$Gini(X_v) = 1-\\sum_{i=1}^{m} p_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme de prédiction\n",
    "\n",
    "La classification des objets se fait par une séquence de tests successifs de l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(alpha=1, activation='relu', solver='sgd',\\\n",
    "                    max_iter=1500)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "plot_decision(X, y ,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apprentissage non supervisé\n",
    "\n",
    "### Clustering\n",
    "\n",
    "Le but principal du clustering est la découverte automatique des structures similaires dans l’espace d’objets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/clusters1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Le processus du clustering vise à construire des groupes (clusters) d’objets similaires à partir d’un ensemble hétérogène d’objets\n",
    "\n",
    "* Chaque cluster issu de ce processus doit vérifier les deux propriétés suivantes :\n",
    "    * **Cohésion interne** : les objets appartenant à ce cluster soient les plus similaires possibles\n",
    "    * **Isolation externe** : les objets appartenant aux autres clusters soient les plus distinctes possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Le processus de clustering repose sur une mesure précise de la similarité des objets que l’on veut regrouper. Cette mesure est appelée distance ou métrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stratégies\n",
    "\n",
    "* Partitionnement (K-means)\n",
    "* Clustering hiérarchique\n",
    "* Clustering basé sur la densité (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithme K-means\n",
    "\n",
    "* L’algorithme partitionne l’ensemble des données à un certain nombre de clusters K (K est fourni par l’utilisateur)\n",
    "* Chaque cluster est représenté par son centre\n",
    "* On commence avec K clusters et on raffine les clusters itérativement\n",
    "* K-means génère une partition Hard (chaque objet appartient à un seul cluster seulement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Algorithme\n",
    "\n",
    "1. Sélectionner aléatoirement un ensemble de K objets comme centres initiaux\n",
    "2. Répéter :\n",
    "    * Former K clusters et ce en assignant chaque point au centre le plus proche\n",
    "    * Recalculer les centres de clusters\n",
    "3. Jusqu’à stabilité de la partition (les centres ne changent pas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/kmeans1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Caractéristiques de K-means\n",
    "\n",
    "* **Avantages**\n",
    "    * Relativement efficace (rapide)\n",
    "    * Converge souvent\n",
    "* **Faiblesses**\n",
    "    * Besoin de spécifier K\n",
    "    * Ne gère pas le bruit\n",
    "    * Sensibles à la sélection initiale des centres de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figs/kmeans2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "#print(kmeans.labels_)\n",
    "#print(kmeans.cluster_centers_)\n",
    "\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Générer la figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:,0],X[:,1], c=y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering hiérarchique\n",
    "\n",
    "* Un algorithme de clustering hiérarchique ne produit pas une seule partition mais une hiérarchie de partition emboîtées\n",
    "* Un cluster est défini comme un noeud d’arbre, auquel est associé l’ensemble des objets qui le composent\n",
    "* Il existe deux catégories d’algorithmes hiérarchiques :\n",
    "     * Méthodes **ascendantes** ou **agglomératives**\n",
    "     * Méthodes **descendantes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Méthodes ascendantes (agglomératives)\n",
    "\n",
    "* La partition initiale contient autant de clusters que d’objets (K = n) \n",
    "* À chaque étape, on cherche un couple (Ci , Cj ) de clusters candidats à la fusion qui maximise une certaine mesure de similarité\n",
    "* On réitère ce processus jusqu’à l’obtention d’un seul cluster contenant tous les éléments\n",
    "* Afin de déterminer le nombre de clusters, on coupe la hiérarchie à un certain niveau\n",
    "\n",
    "\n",
    "<img src=\"figs/clust2.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Méthodes descendantes\n",
    "\n",
    "* Commencer avec un cluster contenant touts objets\n",
    "* Séparer les groupes en plus petits groupes jusqu’à ce que chauqe groupe ne contient qu’un seul objet\n",
    "* Dans cette approche, on a besoin de décider qu’elle est le cluster qu’on doit le diviser, à quelle étape et comment faire la division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering()\n",
    "\n",
    "y_pred = clustering.fit_predict(X)\n",
    "#print(clustering.labels_)\n",
    "\n",
    "# Générer la figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:,0],X[:,1], c=y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering basé sur la densité\n",
    "\n",
    "* Les techniques de clustering vu précédemment ne permettent pas l’identification de clusters de forme : étirée, linéaire, allongée, etc.\n",
    "\n",
    "* DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est capable d’identifier ce type de clusters\n",
    "\n",
    "\n",
    "<img src=\"figs/clust3.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Un cluster est une région de grande densité entourée par des points avec une densité relativement faible\n",
    "* Un bruit appartient à une région de très faible dendité\n",
    "* On dit un objet appartient à une région de forte densité si la cardinalité de son voisinage dépasse un certiain seuil\n",
    "\n",
    "<img src=\"figs/clust4.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.35, min_samples=3)\n",
    "y_pred = clustering.fit_predict(X)\n",
    "#print(y_pred)\n",
    "\n",
    "# Générer la figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:,0],X[:,1], c=y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Évaluation de l’apprentissage\n",
    "\n",
    "### Utilisation d’un échantillon de test\n",
    "\n",
    "* La méthode la plus simple pour estimer la qualité d’un algorithme d’apprentissage est de diviser l’ensemble des exemples en deux ensembles indépendants : le premier, noté $A$, est utilisé pour l’apprentissage, le second, noté $T$, sert à mesurer sa qualité.\n",
    "* $T$ est l’échantillon de test tel que : $S = A ∪ T$ et $A ∩ T = Φ$\n",
    "* La mesure des erreurs commises par l’algorithme d’apprentissage sur l’ensemble de test T est une estimation de sa qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### La validation croisée\n",
    "\n",
    "* Diviser les données d’appentissage $S$ en k sous-échantillons de tailles égales\n",
    "* Répeter\n",
    "    * Retenir l’un de ces échantillons ($i$). \n",
    "    * Entrainer l’algorithme $C$ sur l’ensemble $S − i$\n",
    "    * Mesurer le taux d’erreur $R_i(C)$ sur l’ensemble de test $i$\n",
    "* Recommencer le processus décrit ci-dessus pour chaque échantillon $i$ \n",
    "* L’erreur estimée finale est donnée par la moyenne des erreurs mesurées\n",
    "\n",
    "<img src=\"figs/cross_validation.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "from pprint import pprint\n",
    "\n",
    "scoring = ['precision', 'recall', 'f1']\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=5)\n",
    "\n",
    "pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-one-out\n",
    "\n",
    "Lorsque les données disponibles sont très peu nombreuses, il est possible de pousser à l’extrême la méthode de validation croisée en prenant k le nombre total d’exemple diponible (k = n). Dans ce cas, on ne retient à chaque fois qu’un seul exemple pour le test, et on répète l’apprentissage k fois pour tous les autres exemples d’apprentissage.\n",
    "\n",
    "\n",
    "<img src=\"figs/leave.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "scores = list()\n",
    "\n",
    "for train, test in loo.split(X):\n",
    "    # apprentissage\n",
    "    clf.fit(X[train], y[train])\n",
    "    # prédiction\n",
    "    y_pred = clf.predict(X[test])\n",
    "    # calcul score\n",
    "    scores.append(accuracy_score(y[test], y_pred))\n",
    "\n",
    "scores = np.array(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrice de confusion\n",
    "\n",
    "Mesurer la qualité de généralisation du classificateur\n",
    "\n",
    "![](figs/matrice.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"]\n",
    "y_pred = [\"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"B\"]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calcul des mesures\n",
    "\n",
    "$Rappel = \\frac{TP}{TP + FN}$\n",
    "(par rapport aux instances réelles)\n",
    "\n",
    "$Précision  = \\frac{TP}{TP + FP}$\n",
    "(par rapport aux instances prédites)\n",
    "\n",
    "$F-mesure = \\frac{2 (Precision \\times Rappel)}{Precision + Rappel)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "y_true = [0, 0, 0, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "print(\"Rappel = {}\".format(recall_score(y_true, y_pred)))\n",
    "print(\"Précision = {}\".format(precision_score(y_true, y_pred)))\n",
    "print(\"F-measure = {}\".format(f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**True Positive Rate**\n",
    "(ou sensibilité)\n",
    "\n",
    "$TPR  = \\frac{TP}{TP + FN}$\n",
    "\t\t\t\t\n",
    "\n",
    "**True Negative Rate**\n",
    "(ou Spécificité)\n",
    "\n",
    "$TNR = \\frac{TN}{FP + TN} $\n",
    "\t\t\t\t\n",
    "\n",
    "**False Positive Rate**\n",
    "(ou 1-Spécificité)\n",
    "\n",
    "$FPR  = \\frac{FP}{FP + TN}$ \n",
    "\t\t\t\t\n",
    "\n",
    "**False Negative Rate**\n",
    "\n",
    "$FNR = \\frac{FN}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Courbe ROC\n",
    "\n",
    "* ROC : Receiver Operating Characteristic\n",
    "* Elle met en ralation dans un graphique les taux de faux positifs (en abscisse) et les taux de vraix postifs (en ordonnée)\n",
    "* Elle est définie pour les problémes de deux classes\n",
    "\n",
    "\n",
    "<img src=\"figs/courbe_roc.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quelques points importants dans la courbe :\n",
    "\n",
    "* (FPR, TPR) :\n",
    "* (0, 0) prédit toujours négatif\n",
    "* (1, 1) prédit toujours positif\n",
    "* (0, 1) classification idéale\n",
    "* Ligne diagonale (ligne de hasard) : classification aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# exemple de https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "#y = np.array([1, 1, 2, 2])\n",
    "#scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "#fpr, tpr, thresholds = roc_curve(y, scores, pos_label=1)\n",
    "#print(thresholds)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Instanciation du classifieur\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Apprentissage/entrainement\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "scores = clf.predict_proba(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, scores[:,0], pos_label=0)\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
